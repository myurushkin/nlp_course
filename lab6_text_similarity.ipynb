{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi there! In this notebook we'll review a different methods of searching similar texts! At the end of notebook we'll consider a problem of fast similarity score calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://vene.ro/images/wmd-obama.png' height='600' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, pandas, tqdm, random\n",
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_csv('./training.1600000.processed.noemoticon.csv',names=['id', 'date', 'type', 'author', 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's tweets - small pieces texts. Let's tokenize these texts. Particularly we have to remove hashtags and names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenize_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['tweet'][:100000].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texts)):\n",
    "    texts[i] = tokenize_tweets.tokenize(texts[i].lower()).decode(\"ascii\", errors=\"ignore\").encode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's pretrain word2vec model for computing similarity score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set()\n",
    "for ch in \";!,.?\":\n",
    "    stop.add(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for snt in tqdm.tqdm(sent_tokenize(text)):\n",
    "    snt = [w for w in word_tokenize(snt) if not w in stop]\n",
    "    if len(snt) > 0:\n",
    "        sentences.append(snt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snt in sentences[:10]:\n",
    "    print(\" \".join(snt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(sentences, size=100, max_vocab_size=200000, workers=4, iter=10)\n",
    "w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.similar_by_word('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_snt = \"worry about passing my test\"\n",
    "query_snt = \"i'm ill today\"\n",
    "print(\"Query: \" + query_snt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The esiest way to compare two documents - just average their word2vec embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text):\n",
    "    # convert every word in the text to it's w2vec embedding and average all of them!\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.sum(a * b)/(norm(a) * norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = [(txt, text2vec(txt)) for txt in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_vec = text2vec(query_snt)\n",
    "results = sorted([(snt, -cosine_similarity(vec, query_vec)) for snt, vec in database], key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snt, score in results[:10]:\n",
    "    print(\"{}: {}\".format(snt, str(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll try WMD distance to find sentences which are semantically similar to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = [word_tokenize(txt) for txt in texts[:10000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query_vec = text2vec(query_snt)\n",
    "query_snt_words = query_snt.split()\n",
    "results = sorted([(snt, w2v.wmdistance(query_snt_words, snt)) for snt in database], key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snt, score in results[:10]:\n",
    "    print(\"{}: {}\".format(\" \".join(snt), str(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to be WMD gives promising results. But it's too slow... Let's t try to train a siamese network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://slideplayer.com/12063757/69/images/9/Siamese+network+architecture.jpg' height='600' width='600'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_preprocessed = []\n",
    "for txt in texts[:10000]:\n",
    "    texts_preprocessed.append([w for w in word_tokenize(txt) if not w in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, LSTM, GRU\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(None,w2v.vector_size))\n",
    "out = GRU(256, return_sequences=True)(input)\n",
    "out = LSTM(256)(out)\n",
    "encoder = Model(input, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement siamese neural network.\n",
    "# Let's suppose that texts embeddings (v1 and v2) are compared in the following way  = 1 / (1 + |v1 - v2|)\n",
    "input = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(snt, seq_size):\n",
    "    assert isinstance(snt, str)\n",
    "    snt = snt.split()\n",
    "    result = np.zeros((seq_size, w2v.vector_size))\n",
    "    for i,w in enumerate(snt[:seq_size]):\n",
    "        if w in w2v:\n",
    "            result[i,:] = w2v[w]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(texts, batch_count=64, batch_size=64, seq_size=20):\n",
    "    batch_a = np.zeros((batch_size, seq_size, w2v.vector_size))\n",
    "    batch_b = np.zeros((batch_size, seq_size, w2v.vector_size))\n",
    "    labels = np.zeros((batch_size,))\n",
    "    \n",
    "    for bi in range(batch_count):\n",
    "        for seq_index in range(batch_size):\n",
    "            seq_a, seq_b = random.sample(texts,2)\n",
    "            batch_a[seq_index, :] = encode_text(seq_a, seq_size)\n",
    "            batch_b[seq_index, :] = encode_text(seq_b, seq_size)\n",
    "            dist = w2v.wmdistance(seq_a, seq_b)\n",
    "            labels[seq_index] = 1.0/(1+dist)\n",
    "        yield batch_a, batch_b, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "sentences_train, sentences_test = train_test_split(texts, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    losses = []\n",
    "    for batch_a, batch_b, labels in get_batches(sentences_train):\n",
    "        loss = siamese_model.train_on_batch([batch_a, batch_b], labels)\n",
    "        losses.append(loss)\n",
    "    print(\"train_loss: {}\".format(np.mean(losses)))\n",
    "    \n",
    "    losses = []\n",
    "    for batch_a, batch_b, labels in get_batches(sentences_test):\n",
    "        loss = siamese_model.test_on_batch([batch_a, batch_b], labels)\n",
    "        losses.append(loss)\n",
    "    print(\"test_loss: {}\".format(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = np.array([encode_text(snt, 20) for snt in sentences_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_database = encoder.predict(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vec = encoder.predict(np.expand_dims(encode_text(\"let's go to the sinema !\", 20),0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_database.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. calculate similarity scores vector\n",
    "# 2. select top5 sentences with maximum scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "1. **3 points** tune siamese network. Feel free to perform research. It's highly appreciated if you apply new tricks which were not presented in the scope of this course!\n",
    "2. **7 points** implement/train/test two extra models with  contrastive and triplet losses. Compare their outputs. What's your thoughts about these models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
